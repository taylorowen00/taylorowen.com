---
title: "AI Governance Is a Precondition, Not a Constraint"
date: 2026-02-19
excerpt: "Opening statement before the House Standing Committee on Science and Research on AI, drawing on my submission to the National AI Strategy Task Force."
---

Last week I appeared at a House Standing Committee on Science and Research meeting on AI. Below is text and video of my opening statements, which draws on my submission to the National AI Strategy Task Force.

---

Thank you, Chair and members of the Committee, for the invitation to appear today.

I am Taylor Owen, the Beaverbrook Chair in Media, Ethics and Communication at the Max Bell School of Public Policy at McGill University and the Founding Director of the Centre for Media, Technology and Democracy. I was recently asked to contribute to the federal AI Strategy Task Force on the theme of trust and safety, and my remarks today draw from that submission, which is publicly available.

I understand that this study is focused on AI research: on recent advances, on the needs of research institutions, and on the role of the federal government in promoting a responsible AI research ecosystem. I want to address that final point directly, because I believe it is foundational.

The government has made clear that it wants Canadians to adopt AI. This is understandable. I increasingly believe that this technology is transformational. It has genuine potential to improve productivity, reshape public services, and strengthen our economic competitiveness. To do this, Canada should certainly invest in AI research infrastructure, fund centres of excellence, and recruit world-class talent. But investment alone is not a strategy for adoption. Creating the conditions under which those systems can be safely integrated into public life is equally essential.

If the systems that emerge from Canadian AI research and development are deployed without clear standards for safety, transparency, and accountability, and if the public does not trust them as a result, then the entire enterprise is undermined. A responsible AI research ecosystem requires a responsible AI governance ecosystem. The two cannot be separated.

And right now, that public confidence is not there.

Only 34% of Canadians are willing to trust AI systems. Nearly 80% are concerned about negative outcomes. 78% fear AI will spread false information during elections. 70% are worried about their children accessing AI chatbots. Close to 80% believe AI threatens their job. And 88% want stronger governance.

This is not a literacy gap. Canadians are making reasonable judgments: these systems were deployed without meaningful oversight, and they have seen and felt the consequences. This is a governance gap. And without closing it, neither the adoption the government seeks nor public confidence in AI research will materialize.

My submission to the Task Force argues that closing this gap requires action on three fronts. I think of these as the democratic foundations of AI governance.

The first is citizen safety. AI systems are being deployed at an unprecedented pace, often without meaningful risk assessment. We have seen chatbots fail users in mental health crises and children exposed to harmful content through AI-powered products. Democratic governance requires independent regulatory authority with the power to mandate risk assessments before deployment and ensure heightened protections for children.

The second is information integrity. AI now both curates and creates our information environment. Synthetic media is proliferating, provenance is obscured, and citizens cannot distinguish authentic material from fabrication. Democratic governance requires mandatory AI labeling and provenance disclosure, and data access frameworks that allow researchers to study these systems.

The third is democratic legitimacy. Citizens must have meaningful agency over AI systems that affect their lives: recourse mechanisms when AI causes harm, data portability so users are not locked in, and structured public consultation so governance is shaped by the people it affects.

These principles can be translated into concrete policy. Canada does not need sweeping new AI legislation to act. Most of these measures can be implemented through two existing frameworks: an amended Online Harms Act that brings AI chatbots within scope, and an amended Consumer Privacy Protection Act with AI-specific transparency requirements. Both bills had cross-partisan support. Both could be revived quickly.

I would also add that addressing this governance gap is itself a research challenge. Too often, AI funding is understood narrowly as investment in computer science and technical infrastructure. But understanding AI's effects on society, and developing policy tools to address them, requires sustained investment in social science, humanities, and interdisciplinary research. Canada has strength in this area, but it remains significantly underfunded. If this Committee is considering the needs of Canada's AI research ecosystem, I would urge it to think broadly about what that ecosystem must include.

Governance is not a constraint on a responsible AI research ecosystem. It is a precondition for it.

Thank you. I welcome your questions.
